{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T7_T56Q44EY",
        "outputId": "415f4836-ac94-4b7b-e423-de4a0e549a3b"
      },
      "source": [
        "%tensorflow_version 1.14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K41LsOv7plpG"
      },
      "source": [
        "**Problem 1** Looking back on the scratch\n",
        "\n",
        "\n",
        "*   Had to initialize the weights.\n",
        "*   Needed an epoch loop.\n",
        "*   Train the model with batches.\n",
        "*   Design the network.\n",
        "*   Calculate the forward pass.\n",
        "*   Calculate the loss using the loss function.\n",
        "*   Update the weights (backpropagation).\n",
        "*   Use trained model to infer validation data or test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsh4_RmWy9im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0aa330-aad6-4fe8-acb1-b84d3fe5ba5f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu0MPpInB9gH"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3lCkv-WPFzp"
      },
      "source": [
        "**Problem 2:**\n",
        "Consider the correspondence between scratch and TensorFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVKVN764C13N"
      },
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "#Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert label to number\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY_5KvCSCGks"
      },
      "source": [
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVQKQJZQku_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d48ac4-bed9-4222-d514-609ca02e8b06"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 3.4856, val_loss : 31.9409, acc : 0.750, val_acc : 0.375\n",
            "Epoch 1, loss : 1.6658, val_loss : 11.3622, acc : 0.000, val_acc : 0.000\n",
            "Epoch 2, loss : 1.2576, val_loss : 8.1329, acc : 0.000, val_acc : 0.188\n",
            "Epoch 3, loss : 0.8895, val_loss : 8.0925, acc : 0.000, val_acc : 0.250\n",
            "Epoch 4, loss : 0.6792, val_loss : 6.1259, acc : 0.000, val_acc : 0.250\n",
            "Epoch 5, loss : 0.4080, val_loss : 2.4915, acc : 0.000, val_acc : 0.375\n",
            "Epoch 6, loss : 0.2190, val_loss : 2.1563, acc : 0.000, val_acc : 0.500\n",
            "Epoch 7, loss : 0.1380, val_loss : 1.1806, acc : 0.250, val_acc : 0.625\n",
            "Epoch 8, loss : 0.0667, val_loss : 0.7452, acc : 0.500, val_acc : 0.750\n",
            "Epoch 9, loss : 0.0330, val_loss : 0.4585, acc : 1.000, val_acc : 0.812\n",
            "Epoch 10, loss : 0.0150, val_loss : 0.2696, acc : 1.000, val_acc : 0.875\n",
            "Epoch 11, loss : 0.0089, val_loss : 0.1959, acc : 1.000, val_acc : 0.938\n",
            "Epoch 12, loss : 0.0056, val_loss : 0.1359, acc : 1.000, val_acc : 0.938\n",
            "Epoch 13, loss : 0.0044, val_loss : 0.1107, acc : 1.000, val_acc : 0.938\n",
            "Epoch 14, loss : 0.0037, val_loss : 0.1009, acc : 1.000, val_acc : 0.938\n",
            "Epoch 15, loss : 0.0033, val_loss : 0.0825, acc : 1.000, val_acc : 0.938\n",
            "Epoch 16, loss : 0.0030, val_loss : 0.0726, acc : 1.000, val_acc : 0.938\n",
            "Epoch 17, loss : 0.0028, val_loss : 0.0694, acc : 1.000, val_acc : 0.938\n",
            "Epoch 18, loss : 0.0026, val_loss : 0.0653, acc : 1.000, val_acc : 0.938\n",
            "Epoch 19, loss : 0.0024, val_loss : 0.0608, acc : 1.000, val_acc : 1.000\n",
            "Epoch 20, loss : 0.0023, val_loss : 0.0584, acc : 1.000, val_acc : 1.000\n",
            "Epoch 21, loss : 0.0021, val_loss : 0.0563, acc : 1.000, val_acc : 1.000\n",
            "Epoch 22, loss : 0.0020, val_loss : 0.0537, acc : 1.000, val_acc : 1.000\n",
            "Epoch 23, loss : 0.0019, val_loss : 0.0514, acc : 1.000, val_acc : 1.000\n",
            "Epoch 24, loss : 0.0018, val_loss : 0.0500, acc : 1.000, val_acc : 1.000\n",
            "Epoch 25, loss : 0.0017, val_loss : 0.0489, acc : 1.000, val_acc : 1.000\n",
            "Epoch 26, loss : 0.0017, val_loss : 0.0480, acc : 1.000, val_acc : 1.000\n",
            "Epoch 27, loss : 0.0016, val_loss : 0.0473, acc : 1.000, val_acc : 1.000\n",
            "Epoch 28, loss : 0.0016, val_loss : 0.0465, acc : 1.000, val_acc : 1.000\n",
            "Epoch 29, loss : 0.0015, val_loss : 0.0460, acc : 1.000, val_acc : 1.000\n",
            "Epoch 30, loss : 0.0015, val_loss : 0.0455, acc : 1.000, val_acc : 1.000\n",
            "Epoch 31, loss : 0.0014, val_loss : 0.0449, acc : 1.000, val_acc : 1.000\n",
            "Epoch 32, loss : 0.0014, val_loss : 0.0444, acc : 1.000, val_acc : 1.000\n",
            "Epoch 33, loss : 0.0014, val_loss : 0.0439, acc : 1.000, val_acc : 1.000\n",
            "Epoch 34, loss : 0.0013, val_loss : 0.0433, acc : 1.000, val_acc : 1.000\n",
            "Epoch 35, loss : 0.0013, val_loss : 0.0428, acc : 1.000, val_acc : 1.000\n",
            "Epoch 36, loss : 0.0013, val_loss : 0.0421, acc : 1.000, val_acc : 1.000\n",
            "Epoch 37, loss : 0.0013, val_loss : 0.0415, acc : 1.000, val_acc : 1.000\n",
            "Epoch 38, loss : 0.0012, val_loss : 0.0410, acc : 1.000, val_acc : 1.000\n",
            "Epoch 39, loss : 0.0012, val_loss : 0.0405, acc : 1.000, val_acc : 1.000\n",
            "Epoch 40, loss : 0.0012, val_loss : 0.0400, acc : 1.000, val_acc : 1.000\n",
            "Epoch 41, loss : 0.0012, val_loss : 0.0395, acc : 1.000, val_acc : 1.000\n",
            "Epoch 42, loss : 0.0011, val_loss : 0.0391, acc : 1.000, val_acc : 1.000\n",
            "Epoch 43, loss : 0.0011, val_loss : 0.0389, acc : 1.000, val_acc : 1.000\n",
            "Epoch 44, loss : 0.0011, val_loss : 0.0388, acc : 1.000, val_acc : 1.000\n",
            "Epoch 45, loss : 0.0011, val_loss : 0.0386, acc : 1.000, val_acc : 1.000\n",
            "Epoch 46, loss : 0.0011, val_loss : 0.0382, acc : 1.000, val_acc : 1.000\n",
            "Epoch 47, loss : 0.0010, val_loss : 0.0379, acc : 1.000, val_acc : 1.000\n",
            "Epoch 48, loss : 0.0010, val_loss : 0.0378, acc : 1.000, val_acc : 1.000\n",
            "Epoch 49, loss : 0.0010, val_loss : 0.0375, acc : 1.000, val_acc : 1.000\n",
            "Epoch 50, loss : 0.0010, val_loss : 0.0373, acc : 1.000, val_acc : 1.000\n",
            "Epoch 51, loss : 0.0010, val_loss : 0.0371, acc : 1.000, val_acc : 1.000\n",
            "Epoch 52, loss : 0.0010, val_loss : 0.0368, acc : 1.000, val_acc : 1.000\n",
            "Epoch 53, loss : 0.0010, val_loss : 0.0366, acc : 1.000, val_acc : 1.000\n",
            "Epoch 54, loss : 0.0009, val_loss : 0.0363, acc : 1.000, val_acc : 1.000\n",
            "Epoch 55, loss : 0.0009, val_loss : 0.0358, acc : 1.000, val_acc : 1.000\n",
            "Epoch 56, loss : 0.0009, val_loss : 0.0356, acc : 1.000, val_acc : 1.000\n",
            "Epoch 57, loss : 0.0009, val_loss : 0.0355, acc : 1.000, val_acc : 1.000\n",
            "Epoch 58, loss : 0.0009, val_loss : 0.0353, acc : 1.000, val_acc : 1.000\n",
            "Epoch 59, loss : 0.0009, val_loss : 0.0349, acc : 1.000, val_acc : 1.000\n",
            "Epoch 60, loss : 0.0009, val_loss : 0.0347, acc : 1.000, val_acc : 1.000\n",
            "Epoch 61, loss : 0.0009, val_loss : 0.0345, acc : 1.000, val_acc : 1.000\n",
            "Epoch 62, loss : 0.0008, val_loss : 0.0342, acc : 1.000, val_acc : 1.000\n",
            "Epoch 63, loss : 0.0008, val_loss : 0.0340, acc : 1.000, val_acc : 1.000\n",
            "Epoch 64, loss : 0.0008, val_loss : 0.0338, acc : 1.000, val_acc : 1.000\n",
            "Epoch 65, loss : 0.0008, val_loss : 0.0336, acc : 1.000, val_acc : 1.000\n",
            "Epoch 66, loss : 0.0008, val_loss : 0.0334, acc : 1.000, val_acc : 1.000\n",
            "Epoch 67, loss : 0.0008, val_loss : 0.0331, acc : 1.000, val_acc : 1.000\n",
            "Epoch 68, loss : 0.0008, val_loss : 0.0329, acc : 1.000, val_acc : 1.000\n",
            "Epoch 69, loss : 0.0008, val_loss : 0.0327, acc : 1.000, val_acc : 1.000\n",
            "Epoch 70, loss : 0.0008, val_loss : 0.0325, acc : 1.000, val_acc : 1.000\n",
            "Epoch 71, loss : 0.0008, val_loss : 0.0322, acc : 1.000, val_acc : 1.000\n",
            "Epoch 72, loss : 0.0008, val_loss : 0.0321, acc : 1.000, val_acc : 1.000\n",
            "Epoch 73, loss : 0.0007, val_loss : 0.0319, acc : 1.000, val_acc : 1.000\n",
            "Epoch 74, loss : 0.0007, val_loss : 0.0317, acc : 1.000, val_acc : 1.000\n",
            "Epoch 75, loss : 0.0007, val_loss : 0.0316, acc : 1.000, val_acc : 1.000\n",
            "Epoch 76, loss : 0.0007, val_loss : 0.0314, acc : 1.000, val_acc : 1.000\n",
            "Epoch 77, loss : 0.0007, val_loss : 0.0312, acc : 1.000, val_acc : 1.000\n",
            "Epoch 78, loss : 0.0007, val_loss : 0.0310, acc : 1.000, val_acc : 1.000\n",
            "Epoch 79, loss : 0.0007, val_loss : 0.0309, acc : 1.000, val_acc : 1.000\n",
            "Epoch 80, loss : 0.0007, val_loss : 0.0307, acc : 1.000, val_acc : 1.000\n",
            "Epoch 81, loss : 0.0007, val_loss : 0.0305, acc : 1.000, val_acc : 1.000\n",
            "Epoch 82, loss : 0.0007, val_loss : 0.0303, acc : 1.000, val_acc : 1.000\n",
            "Epoch 83, loss : 0.0007, val_loss : 0.0301, acc : 1.000, val_acc : 1.000\n",
            "Epoch 84, loss : 0.0007, val_loss : 0.0299, acc : 1.000, val_acc : 1.000\n",
            "Epoch 85, loss : 0.0007, val_loss : 0.0297, acc : 1.000, val_acc : 1.000\n",
            "Epoch 86, loss : 0.0007, val_loss : 0.0295, acc : 1.000, val_acc : 1.000\n",
            "Epoch 87, loss : 0.0006, val_loss : 0.0294, acc : 1.000, val_acc : 1.000\n",
            "Epoch 88, loss : 0.0006, val_loss : 0.0292, acc : 1.000, val_acc : 1.000\n",
            "Epoch 89, loss : 0.0006, val_loss : 0.0290, acc : 1.000, val_acc : 1.000\n",
            "Epoch 90, loss : 0.0006, val_loss : 0.0288, acc : 1.000, val_acc : 1.000\n",
            "Epoch 91, loss : 0.0006, val_loss : 0.0286, acc : 1.000, val_acc : 1.000\n",
            "Epoch 92, loss : 0.0006, val_loss : 0.0284, acc : 1.000, val_acc : 1.000\n",
            "Epoch 93, loss : 0.0006, val_loss : 0.0282, acc : 1.000, val_acc : 1.000\n",
            "Epoch 94, loss : 0.0006, val_loss : 0.0281, acc : 1.000, val_acc : 1.000\n",
            "Epoch 95, loss : 0.0006, val_loss : 0.0279, acc : 1.000, val_acc : 1.000\n",
            "Epoch 96, loss : 0.0006, val_loss : 0.0277, acc : 1.000, val_acc : 1.000\n",
            "Epoch 97, loss : 0.0006, val_loss : 0.0274, acc : 1.000, val_acc : 1.000\n",
            "Epoch 98, loss : 0.0006, val_loss : 0.0271, acc : 1.000, val_acc : 1.000\n",
            "Epoch 99, loss : 0.0006, val_loss : 0.0268, acc : 1.000, val_acc : 1.000\n",
            "test_acc : 0.950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5vxGPousGsC"
      },
      "source": [
        "*   Had to initialize the weights.\n",
        "    ```\n",
        "    init = tf.global_variables_initializer()\n",
        "    ```\n",
        "\n",
        "*   Needed an epoch loop.\n",
        "    ```\n",
        "    for epoch in range(num_epochs):\n",
        "    ```\n",
        "\n",
        "*   Train the model with batches.\n",
        "    ```\n",
        "    define the GetMiniBatch class\n",
        "    ```\n",
        "\n",
        "*   Design the network.\n",
        "    ```\n",
        "    define the example_net function\n",
        "    ```\n",
        "\n",
        "*   Calculate the forward pass.\n",
        "    ```\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    ```\n",
        "\n",
        "*   Calculate the loss using the loss function.\n",
        "    ```\n",
        "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logit(labels=Y, logits=logits))\n",
        "    ```\n",
        "\n",
        "*   Update the weights (backpropagation):\n",
        "    ```\n",
        "    loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "    ```\n",
        "\n",
        "*   Use trained model to infer validation data or test data.\n",
        "    ```\n",
        "    val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    ...\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPR77KKNxqVH"
      },
      "source": [
        "**Problem 3** Create a model of Iris using all three types of objective variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7BmEx7PMHR"
      },
      "source": [
        "# Training on 3 classes\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "#Condition extraction from data frame\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Convert label to number\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y[y == \"Iris-setosa\"] = 2\n",
        "\n",
        "# Convert label to one-hot vector\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y = enc.fit_transform(y[:, np.newaxis])\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gq3LQlkCvi1"
      },
      "source": [
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pooYI435zBcG"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCctHNoHDJr2"
      },
      "source": [
        "**Problem 4** Creating a model of House Prices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhAujUWGC4aJ"
      },
      "source": [
        "# Load House Prices dataset\n",
        "df_house = pd.read_csv('train.csv')\n",
        "\n",
        "y = df_house[['SalePrice']]\n",
        "X = df_house[['GrLivArea', 'YearBuilt']]\n",
        "y = np.array(np.log1p(y))\n",
        "X = np.array(np.log1p(X))\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OABzIn3YHJO6"
      },
      "source": [
        "def regression_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItjsAP0LHZsx",
        "outputId": "b886b1d6-57fa-40c0-a9a5-9cbec9060492"
      },
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = regression_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.square(Y - logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "#Index value calculation\n",
        "mean_square_error = tf.reduce_mean(tf.square(Y - logits))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_mse = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, mse = sess.run([loss_op, mean_square_error], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_mse += mse\n",
        "        total_loss /= n_samples\n",
        "        total_mse /= n_samples\n",
        "        val_loss, val_mse = sess.run([loss_op, mean_square_error], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, mse : {:.3f}, val_mse : {:.3f}\".format(epoch, total_loss, val_loss, mse, val_mse))\n",
        "    test_mse = sess.run(mean_square_error, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_mse : {:.3f}\".format(test_mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 205.8077, val_loss : 3.5418, mse : 3.646, val_mse : 3.542\n",
            "Epoch 1, loss : 0.2607, val_loss : 1.4297, mse : 1.490, val_mse : 1.430\n",
            "Epoch 2, loss : 0.0902, val_loss : 0.4609, mse : 0.673, val_mse : 0.461\n",
            "Epoch 3, loss : 0.0298, val_loss : 0.1546, mse : 0.233, val_mse : 0.155\n",
            "Epoch 4, loss : 0.0148, val_loss : 0.1194, mse : 0.075, val_mse : 0.119\n",
            "Epoch 5, loss : 0.0154, val_loss : 0.2364, mse : 0.131, val_mse : 0.236\n",
            "Epoch 6, loss : 0.0151, val_loss : 0.1928, mse : 0.096, val_mse : 0.193\n",
            "Epoch 7, loss : 0.0121, val_loss : 0.1110, mse : 0.043, val_mse : 0.111\n",
            "Epoch 8, loss : 0.0114, val_loss : 0.1044, mse : 0.038, val_mse : 0.104\n",
            "Epoch 9, loss : 0.0117, val_loss : 0.1694, mse : 0.077, val_mse : 0.169\n",
            "test_mse : 0.164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myx02r80Khul"
      },
      "source": [
        "**Problem 5** Creating a MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL1F0B96JguZ"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X, y), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB_wJM0oKvuT"
      },
      "source": [
        "# Smoothing\n",
        "X = X.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ncta5-PK5qW"
      },
      "source": [
        "# Type conversion, normalization\n",
        "X = X.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dKFeSTzLKfV"
      },
      "source": [
        "# Convert label to one-hot vector\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_one_hot = enc.fit_transform(y[:,np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:,np.newaxis])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWvO5q4KQGIt"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjxc9E-1Lefh"
      },
      "source": [
        "def cnn(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([5,5,1,6])),\n",
        "        'w2': tf.Variable(tf.random_normal([5,5,6,16])),\n",
        "        'w3': tf.Variable(tf.random_normal([7*7*16, 120])),\n",
        "        'w4': tf.Variable(tf.random_normal([120, 84])),\n",
        "        'w5': tf.Variable(tf.random_normal([84, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([6])),\n",
        "        'b2': tf.Variable(tf.random_normal([16])),\n",
        "        'b3': tf.Variable(tf.random_normal([120])),\n",
        "        'b4': tf.Variable(tf.random_normal([84])),\n",
        "        'b5': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    x = tf.reshape(x, [-1, 28 , 28 , 1])\n",
        "\n",
        "    conve_1 = tf.add(tf.nn.conv2d(x, weights['w1'], strides=[1,1,1,1], padding='SAME'), biases['b1'])\n",
        "    conve_1 = tf.nn.relu(conve_1)\n",
        "\n",
        "    pooli_1 = tf.nn.pool(conve_1, window_shape=[2,2], strides=[2,2], pooling_type='MAX', padding='VALID')\n",
        "\n",
        "    conve_2 = tf.add(tf.nn.conv2d(pooli_1, weights['w2'], strides=[1,1,1,1], padding='SAME'), biases['b2'])\n",
        "    conve_2 = tf.nn.relu(conve_2)\n",
        "\n",
        "    pooli_2 = tf.nn.pool(conve_2, window_shape=[2,2], strides=[2,2], pooling_type='MAX', padding='VALID')\n",
        "\n",
        "    x = tf.reshape(pooli_2, [-1, 7*7*16])\n",
        "    \n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w3']), biases['b3'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w4']), biases['b4'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w5']) + biases['b5'] # tf.add and + are equivalent\n",
        "    return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hUeNqTnONeZ",
        "outputId": "125d4b5f-ea25-428a-9514-dc3eba424e65"
      },
      "source": [
        "  # Hyperparameter settings\n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = cnn(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 6.4983, val_loss : 104.9089, acc : 0.920, val_acc : 0.888\n",
            "Epoch 1, loss : 0.6259, val_loss : 48.2709, acc : 0.930, val_acc : 0.919\n",
            "Epoch 2, loss : 0.3027, val_loss : 31.9755, acc : 0.940, val_acc : 0.935\n",
            "Epoch 3, loss : 0.1812, val_loss : 22.8258, acc : 0.960, val_acc : 0.941\n",
            "Epoch 4, loss : 0.1185, val_loss : 19.7515, acc : 0.940, val_acc : 0.941\n",
            "Epoch 5, loss : 0.0849, val_loss : 15.6083, acc : 0.960, val_acc : 0.948\n",
            "Epoch 6, loss : 0.0589, val_loss : 15.2744, acc : 0.930, val_acc : 0.945\n",
            "Epoch 7, loss : 0.0443, val_loss : 12.4619, acc : 0.980, val_acc : 0.957\n",
            "Epoch 8, loss : 0.0346, val_loss : 11.6242, acc : 0.960, val_acc : 0.953\n",
            "Epoch 9, loss : 0.0288, val_loss : 8.7738, acc : 0.960, val_acc : 0.960\n",
            "Epoch 10, loss : 0.0239, val_loss : 8.4185, acc : 0.970, val_acc : 0.958\n",
            "Epoch 11, loss : 0.0190, val_loss : 7.7643, acc : 0.970, val_acc : 0.961\n",
            "Epoch 12, loss : 0.0153, val_loss : 7.8986, acc : 0.980, val_acc : 0.954\n",
            "Epoch 13, loss : 0.0146, val_loss : 8.0818, acc : 0.990, val_acc : 0.957\n",
            "Epoch 14, loss : 0.0113, val_loss : 5.8506, acc : 0.990, val_acc : 0.963\n",
            "Epoch 15, loss : 0.0106, val_loss : 5.4468, acc : 0.990, val_acc : 0.960\n",
            "Epoch 16, loss : 0.0081, val_loss : 5.4472, acc : 0.970, val_acc : 0.963\n",
            "Epoch 17, loss : 0.0066, val_loss : 6.3052, acc : 0.950, val_acc : 0.949\n",
            "Epoch 18, loss : 0.0070, val_loss : 4.2916, acc : 0.990, val_acc : 0.962\n",
            "Epoch 19, loss : 0.0059, val_loss : 4.0689, acc : 0.980, val_acc : 0.958\n",
            "Epoch 20, loss : 0.0049, val_loss : 3.0601, acc : 0.990, val_acc : 0.961\n",
            "Epoch 21, loss : 0.0047, val_loss : 3.4692, acc : 0.980, val_acc : 0.956\n",
            "Epoch 22, loss : 0.0037, val_loss : 2.4742, acc : 0.990, val_acc : 0.963\n",
            "Epoch 23, loss : 0.0030, val_loss : 1.7267, acc : 1.000, val_acc : 0.969\n",
            "Epoch 24, loss : 0.0025, val_loss : 1.3183, acc : 1.000, val_acc : 0.969\n",
            "Epoch 25, loss : 0.0017, val_loss : 1.1821, acc : 0.980, val_acc : 0.969\n",
            "Epoch 26, loss : 0.0023, val_loss : 1.0807, acc : 1.000, val_acc : 0.970\n",
            "Epoch 27, loss : 0.0016, val_loss : 1.2149, acc : 0.980, val_acc : 0.967\n",
            "Epoch 28, loss : 0.0010, val_loss : 0.9807, acc : 0.990, val_acc : 0.967\n",
            "Epoch 29, loss : 0.0009, val_loss : 0.6942, acc : 0.990, val_acc : 0.970\n",
            "Epoch 30, loss : 0.0010, val_loss : 0.7836, acc : 0.980, val_acc : 0.967\n",
            "Epoch 31, loss : 0.0007, val_loss : 0.4856, acc : 0.990, val_acc : 0.967\n",
            "Epoch 32, loss : 0.0006, val_loss : 0.4470, acc : 0.990, val_acc : 0.970\n",
            "Epoch 33, loss : 0.0005, val_loss : 0.3869, acc : 0.990, val_acc : 0.973\n",
            "Epoch 34, loss : 0.0004, val_loss : 0.4133, acc : 1.000, val_acc : 0.969\n",
            "Epoch 35, loss : 0.0005, val_loss : 0.2490, acc : 0.980, val_acc : 0.973\n",
            "Epoch 36, loss : 0.0005, val_loss : 0.3050, acc : 0.990, val_acc : 0.974\n",
            "Epoch 37, loss : 0.0004, val_loss : 0.2874, acc : 0.980, val_acc : 0.969\n",
            "Epoch 38, loss : 0.0005, val_loss : 0.2063, acc : 1.000, val_acc : 0.970\n",
            "Epoch 39, loss : 0.0007, val_loss : 0.2044, acc : 1.000, val_acc : 0.974\n",
            "Epoch 40, loss : 0.0004, val_loss : 0.2426, acc : 0.990, val_acc : 0.965\n",
            "Epoch 41, loss : 0.0007, val_loss : 0.2159, acc : 1.000, val_acc : 0.973\n",
            "Epoch 42, loss : 0.0006, val_loss : 0.2849, acc : 0.990, val_acc : 0.969\n",
            "Epoch 43, loss : 0.0006, val_loss : 0.2025, acc : 1.000, val_acc : 0.972\n",
            "Epoch 44, loss : 0.0004, val_loss : 0.1776, acc : 0.980, val_acc : 0.974\n",
            "Epoch 45, loss : 0.0005, val_loss : 0.1952, acc : 1.000, val_acc : 0.972\n",
            "Epoch 46, loss : 0.0006, val_loss : 0.2889, acc : 0.980, val_acc : 0.950\n",
            "Epoch 47, loss : 0.0007, val_loss : 0.1845, acc : 0.990, val_acc : 0.973\n",
            "Epoch 48, loss : 0.0006, val_loss : 0.1749, acc : 0.990, val_acc : 0.968\n",
            "Epoch 49, loss : 0.0008, val_loss : 0.3301, acc : 0.990, val_acc : 0.968\n",
            "test_acc : 0.967\n"
          ]
        }
      ]
    }
  ]
}